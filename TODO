CricketEnv:
 - @FATTO riguarda il constructor, particularly hte action_space and observation_space --> in reset fai previous_action
 - @FATTO implement the step + reward function 
 - @FATTO give a look to the method reset()
 - @FATTO implmement (copia) render
 - @FATTO implement (copia) close

Goal: --> suggerimento: guarda alle funzioni che ho scritto per finta nell'env
 - @FATTO in order to compute the reward function, we need to implement tha class GOAL,
   which can be represented as another class Cricket (not environment) with a certain
   angle for the joints

Neural Networks:
 - @FATTO need to implement both the actor and critic Networks



Traceback (most recent call last):
  File "/home/andrea/Desktop/project/gym-cricket-robot/src/main.py", line 56, in <module>
    ddpg.update(batch_size)
  File "/home/andrea/Desktop/project/gym-cricket-robot/src/DDPG.py", line 68, in update
    next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)
ValueError: expected sequence of length 3 at dim 2 (got 16)