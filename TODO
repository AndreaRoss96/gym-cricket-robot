CricketEnv:
 - @FATTO riguarda il constructor, particularly hte action_space and observation_space --> in reset fai previous_action
 - @FATTO implement the step + reward function 
 - @FATTO give a look to the method reset()
 - @FATTO implmement (copia) render
 - @FATTO implement (copia) close

Goal: --> suggerimento: guarda alle funzioni che ho scritto per finta nell'env
 - @FATTO in order to compute the reward function, we need to implement tha class GOAL,
   which can be represented as another class Cricket (not environment) with a certain
   angle for the joints

Neural Networks:
 - @FATTO need to implement both the actor and critic Networks



 tensor([[ 0.0000,  0.0000,  0.5000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  2.5580, -0.2983,  0.4354,  2.6878,
          0.3689,  0.3752,  2.5084,  2.5029, -0.1806, -2.6925, -1.3508, -1.3173,
         -0.1104, -0.4398, -1.9858, -1.8651,  2.3336, -0.8553, -1.6961,  0.8093,
          0.7123, -2.8393,  1.1209,  2.2082,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')

tensor([[[[[[ 15.]]],


          [[[  0.]]],


          [[[ 15.]]]],



         [[[[ 15.]]],


          [[[  0.]]],


          [[[-15.]]]],



         [[[[-15.]]],


          [[[  0.]]],


          [[[-15.]]]],



         [[[[-15.]]],


          [[[  0.]]],


          [[[ 15.]]]]]], device='cuda:0')