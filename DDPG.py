from gym_cricket.envs.cricket_env import CricketEnv
from neural_network.actor_nn import Actor_nn

# https://spinningup.openai.com/en/latest/algorithms/ddpg.html#:~:text=Background,-(Previously%3A%20Introduction%20to&text=Deep%20Deterministic%20Policy%20Gradient%20(DDPG,function%20to%20learn%20the%20policy.
# https://github.com/ghliu/pytorch-ddpg/blob/master/ddpg.py
def DDPG_algorith(episodes:int):
    '''
    Initialize actor and critic networks ğ’œ ğ‘ , ğ‘’|ğœƒ ğ’œ ğ‘ğ‘›ğ‘‘ ğ’ ğ‘ , ğ‘, ğ‘’|ğœƒğ’ do: with random weights ğœƒ ğ’œ ğ‘ğ‘›ğ‘‘ ğœƒğ’
    Initialize target network ğ’œâ€²ğ‘ğ‘›ğ‘‘ ğ’â€² with random weights ğœƒ ğ’œâ€² â† ğœƒ ğ’œ ğ‘ğ‘›ğ‘‘ ğœƒğ’â€² â† ğœƒğ’
    Initialize replay buffer â„›
    '''
    actor, critic, actor_target, critic_target = init_nn()
    buffer = init_buffer()
    done = False
    cricket_env = CricketEnv()
    for n in range(episodes) :
        '''
        Procedure RobotInit
        Generate a random stable starting state ğ‘  in the unstructured environment
        end procedure RobotInit
        '''
        cricket_env.reset() # reset robot position (randomly) and restart the simulation
        while not done:
            current_state = cricket_env.get
            action = actor.generate_action()
            reward, new_state = cricket_env.step()
            buffer.add((current_state,action,reward,new_state))
            minibatch = buffer.get_minibatch()
            # sample a random minibatch of N transitions
            for (state_i,action_i,reward_i,new_state_i) in minibatch:



